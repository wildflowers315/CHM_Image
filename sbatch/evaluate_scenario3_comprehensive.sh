#!/bin/bash
#SBATCH --job-name=eval_scenario3_comp
#SBATCH --output=logs/%j_evaluate_scenario3_comprehensive.txt
#SBATCH --error=logs/%j_evaluate_scenario3_comprehensive_error.txt
#SBATCH --time=0-2:00:00
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --mem-per-cpu=48G
#SBATCH --partition=main

# Create output directory
mkdir -p logs chm_outputs/scenario3_comprehensive_evaluation

# Activate environment
source chm_env/bin/activate

echo "ğŸš€ Starting Scenario 3A vs 3B Evaluation at $(date)"
echo "ğŸ“Š Comparing:"
echo "   â€¢ Scenario 3A: Tochigi from-scratch GEDI + Fixed Ensemble"  
echo "   â€¢ Scenario 3B: Tochigi fine-tuned GEDI + Fixed Ensemble"

# Evaluate Scenario 3A vs 3B (Fine-tuning vs From-scratch)
echo ""
echo "=== Evaluating Scenario 3A vs 3B ==="
python evaluate_google_embedding_scenario1.py \
    --google-embedding-dir chm_outputs/google_embedding_scenario3b_predictions \
    --original-mlp-dir chm_outputs/google_embedding_scenario3a_predictions \
    --downloads-dir downloads \
    --output-dir chm_outputs/scenario3_comprehensive_evaluation \
    --no-bias-correction \
    --max-patches 63

echo "âœ… Comprehensive evaluation completed at $(date)"

# Show results summary
echo ""
echo "ğŸ“Š Results Summary:"
echo "ğŸ“ Evaluation results saved in: chm_outputs/scenario3_comprehensive_evaluation/"

echo ""
echo "ğŸ¯ Key Comparison Focus:"
echo "   â€¢ Scenario 3A: GEDI trained from scratch on Tochigi"
echo "   â€¢ Scenario 3B: GEDI fine-tuned from Scenario 2A on Tochigi"
echo "   â€¢ Both use fixed ensemble MLP (no retraining)"
echo "   â€¢ Question: Is fine-tuning better than from-scratch for target region?"

echo ""
echo "ğŸ“ˆ Expected Findings:"
echo "   â€¢ Training validation: 3B had better validation loss (8.3112 vs 8.4774)"
echo "   â€¢ Cross-region performance: Compare RÂ², RMSE, correlation across regions"
echo "   â€¢ Tochigi target region: Should show best performance for both scenarios"

echo "ğŸ‰ Scenario 3A vs 3B Evaluation Pipeline Completed!"
echo "â° Completed at: $(date)"